{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e78d87",
   "metadata": {},
   "source": [
    "# Use Apriori analysis to find phrases, or interesting patterns in a novel.\n",
    "\n",
    "Use the nltk library corpus gutenberg API and load the novel 'carroll-alice.txt' which is the Alice in Wonderland by L. Carroll. Use any means to parse/extract words and save in CSV format to be read by Weka framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d7bac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import csv\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a427eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = nltk.corpus.gutenberg.sents('carroll-alice.txt')\n",
    "alice_words = nltk.corpus.gutenberg.words('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094b64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_words = np.array(alice_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc18a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6037368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TermsSentences = []\n",
    "for terms in alice:\n",
    "    terms = [w for w in terms if w not in stop_words]\n",
    "    terms = [w for w in terms if re.search(r'^[a-zA-Z]{2}', w) is not None]\n",
    "    TermsSentences.append(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8080e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_df = pd.DataFrame(TermsSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc6e7e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_csv = alice_df.to_csv('alice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fe7c883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=4497 items, N=1704 transactions\n"
     ]
    }
   ],
   "source": [
    "Transactions_list = []  # a list of transactions\n",
    "Items_names = {}  # Lookup item ID to name\n",
    "Items_ids = {}  # Lookup item name to ID\n",
    "\n",
    "Items = None  # a list of item IDs, normally an increasing sequence of numbers\n",
    "\n",
    "# Process the data\n",
    "with open('alice.csv', 'r') as fin:\n",
    "    reader = csv.reader(fin, delimiter=',')\n",
    "    item_id = 0\n",
    "    for row in reader:\n",
    "        transaction = []\n",
    "        for item in row:\n",
    "            if item not in Items_ids:\n",
    "                Items_ids[item] = item_id\n",
    "                Items_names[item_id] = item\n",
    "                item_id += 1\n",
    "            #\n",
    "            transaction += [Items_ids[item]]\n",
    "        #\n",
    "        Transactions_list += [transaction]\n",
    "\n",
    "M, N = len(Items_ids), len(Transactions_list)\n",
    "\n",
    "Items = np.arange(0,M)\n",
    "\n",
    "# Information\n",
    "print(f'M={M} items, N={N} transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ea3b733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "Transactions = np.full((N,M), False, dtype=bool)\n",
    "\n",
    "for i, t in enumerate(Transactions_list):\n",
    "    for item in t:\n",
    "        Transactions[i][item] = True\n",
    "\n",
    "# Sanity, print row index 10, 11\n",
    "print(f'{Transactions[10:12].astype(int)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90f8a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filename = 'alice.csv'\n",
    "\n",
    "with open(Filename, 'w') as fout:\n",
    "    writer = csv.writer(fout, delimiter=',', quoting=csv.QUOTE_ALL, quotechar=\"'\", lineterminator='\\n')\n",
    "    writer.writerow([Items_names[i] for i in range(M)])\n",
    "    for i in range(N):\n",
    "        writer.writerow(list(map(lambda x: '' if x == False else 'True',  Transactions[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36687f36",
   "metadata": {},
   "source": [
    "## Interesting Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0e58ef",
   "metadata": {},
   "source": [
    "A lowerBoundMinimumSupport of 0.004 produced 20 rules and a lowerBoundMinimumSupport of 0.003 produces 38 rules.\n",
    "\n",
    "Everytime the word \"golden\" appears the word \"little\" also appears.\n",
    "\n",
    "The word \"said\" commonly occurs with \"turtle\" and \"hare.\"\n",
    "\n",
    "When the word \"join\" appears, so does the word \"dance.\"\n",
    "\n",
    "Alice does not appear in the rules until the lowerBoundMinimumSupport is decreased to 0.003.\n",
    "\n",
    "As more rules are produced, more common words appear, such as \"went\", \"would\", and \"said.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cdd497",
   "metadata": {},
   "source": [
    "# Two Hidden Layer Network\n",
    "\n",
    "In the lecture module, the class NeuralNetMLP is a single hidden layer neural network implementation. Make the necessary modifications to upgrade it to a 2 hidden layer network. Run it on the MNIST dataset and report its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "649fcae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows= 60000, columns= 28\n",
      "Rows= 10000, columns= 28\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(f'Rows= {X_train.shape[0]}, columns= {X_train.shape[1]}')\n",
    "print(f'Rows= {X_test.shape[0]}, columns= {X_test.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f9dcdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows= 60000, columns= 784\n",
      "Rows= 10000, columns= 784\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((60000, 784))\n",
    "X_test = X_test.reshape((10000, 784))\n",
    "\n",
    "print(f'Rows= {X_train.shape[0]}, columns= {X_train.shape[1]}')\n",
    "print(f'Rows= {X_test.shape[0]}, columns= {X_test.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87458ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetMLP(object):\n",
    "\n",
    "    def __init__(self, n_hidden=30, n_hidden2 = 60, epochs=100, eta=0.001, minibatch_size=1, seed=None):\n",
    "        self.random = np.random.RandomState(seed)  # used to randomize weights\n",
    "        self.n_hidden = n_hidden  # size of the hidden layer\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.epochs = epochs  # number of iterations\n",
    "        self.eta = eta  # learning rate\n",
    "        self.minibatch_size = minibatch_size  # size of training batch - 1 would not work\n",
    "    \n",
    "    @staticmethod\n",
    "    def onehot(y, n_classes):  # one hot encode the input class y\n",
    "        onehot = np.zeros((n_classes, y.shape[0]))\n",
    "        for idx, val in enumerate(y.astype(int)):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot.T\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):  # Eq 1\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(z, -250, 250)))\n",
    "\n",
    "    def _forward(self, X):  # Eq 2       (takes in batch from training test)\n",
    "        z_h = np.dot(X, self.w_h)\n",
    "        a_h = self.sigmoid(z_h)\n",
    "        z_h2 = np.dot(a_h, self.w_h2)\n",
    "        a_h2 = self.sigmoid(z_h2)\n",
    "        z_out = np.dot(a_h2, self.w_out)\n",
    "        a_out = self.sigmoid(z_out)\n",
    "        return z_h, a_h, z_h2, a_h2, z_out, a_out\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_cost(y_enc, output):  # Eq 4\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0-y_enc) * np.log(1.0-output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        return cost\n",
    "\n",
    "    def predict(self, X):\n",
    "        z_h, a_h, z_h2, a_h2, z_out, a_out = self._forward(X)\n",
    "        y_pred = np.argmax(z_out, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        import sys\n",
    "        n_output = np.unique(y_train).shape[0]  # number of class labels\n",
    "        n_features = X_train.shape[1]\n",
    "        self.w_out = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden2, n_output)) #output weights\n",
    "        self.w_h = self.random.normal(loc=0.0, scale=0.1, size=(n_features, self.n_hidden)) #hidden layer weights\n",
    "        self.w_h2 = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden, self.n_hidden2)) #2nd hidden layer weights\n",
    "        y_train_enc = self.onehot(y_train, n_output)  # one-hot encode original y\n",
    "        for i in range(self.epochs):\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "                z_h, a_h, z_h2, a_h2, z_out, a_out = self._forward(X_train[batch_idx]) #(forward propagation)\n",
    "                sigmoid_derivative_h2 = a_h2 * (1.0-a_h2)  # Eq 3 (sigmoid function derivative)\n",
    "                sigmoid_derivative_h = a_h * (1.0-a_h)  # Eq 3 (sigmoid function derivative)\n",
    "                delta_out = a_out - y_train_enc[batch_idx]  # Eq 5 (backpropagate error) (predicted - actual)\n",
    "                delta_h2 = (np.dot(delta_out, self.w_out.T) * sigmoid_derivative_h2)  # Eq 6 (hidden layer error matrix)\n",
    "                delta_h = (np.dot(delta_h2, self.w_h2.T) * sigmoid_derivative_h)  # Eq 6\n",
    "                grad_w_out = np.dot(a_h2.T, delta_out)  # Eq 7 (loss gradient)\n",
    "                grad_w_h2 = np.dot(a_h.T, delta_h2)  # Eq 8\n",
    "                grad_w_h = np.dot(X_train[batch_idx].T, delta_h)  # Eq 8\n",
    "                self.w_out -= self.eta*grad_w_out  # Eq 9\n",
    "                self.w_h2 -= self.eta*grad_w_h2  # Eq 9\n",
    "                self.w_h -= self.eta*grad_w_h  # Eq 9\n",
    "\n",
    "            # Evaluation after each epoch during training\n",
    "            z_h, a_h, z_h2, a_h2, z_out, a_out = self._forward(X_train)\n",
    "            cost = self.compute_cost(y_enc=y_train_enc, output=a_out)\n",
    "            y_train_pred = self.predict(X_train)  # monitoring training progress through reclassification\n",
    "            y_valid_pred = self.predict(X_valid)  # monitoring training progress through validation\n",
    "            train_acc = ((np.sum(y_train == y_train_pred)).astype(float) / X_train.shape[0])\n",
    "            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(float) / X_valid.shape[0])\n",
    "            sys.stderr.write('\\r%d/%d | Cost: %.2f ' '| Train/Valid Acc.: %.2f%%/%.2f%% '%\n",
    "                (i+1, self.epochs, cost, train_acc*100, valid_acc*100))\n",
    "            sys.stderr.flush()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87418328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300/300 | Cost: 26000.77 | Train/Valid Acc.: 91.85%/93.16% "
     ]
    }
   ],
   "source": [
    "# Define and fit the neural network\n",
    "nn = NeuralNetMLP(n_hidden=20, n_hidden2=40, epochs=300, eta=0.0005, minibatch_size=100, seed=1)\n",
    "\n",
    "nn.fit(X_train=X_train[:55000], y_train=y_train[:55000], X_valid=X_train[55000:], y_valid=y_train[55000:]) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3985a88",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75035fd7",
   "metadata": {},
   "source": [
    "1 hidden layer cost = 30051, training accuracy = 91.21%, validation accuracy = 92.96%\n",
    "\n",
    "2 hidden layer cost = 26000, training accuracy = 91.85%, validation accuracy = 93.16%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75f5ce",
   "metadata": {},
   "source": [
    "Adding a second hidden layer improved both the training and validation accuracy by a very small amount.  The benefit likely does not outweigh the additional cost of adding a layer in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
