{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Describe the environment in the Nim learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment in the Nim learning model consists of a given number of piles.  There is a maximum number of items each pile can contain.  This example has three piles and each of these can exist in 11 different states (0 to 10 items in each).  The piles and their contents make up the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Describe the agent(s) in the Nim learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agents in the Nim learning model are the two players in each game.  This could be any combination of the random player, the Guru, and the Q-Learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Describe the reward and penalty in the Nim learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nim learning model could use reward and penalty at a few different places.  First, the learning model could received a reward when it wins a game and a penalty when it loses a game.  Next, the individual steps that led to a win or a loss could receive a reward or penalty.  For example, when a player removes the last items from all of the piles, a reward would be given.  If the next move led to a win, a penalty would be given since this means the original player lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: How many possible states there could be in the Nim game with a maximum of 10 items per pile and 3 piles total?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be 0 to 10 items in each of the 3 piles at any given time.  This means there are 11^3 possible states.  11^3 = 1331.\n",
    "\n",
    "**Possible States = 1331**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: How many possible unique actions there could be for player 1 in the Nim game with 10 items per pile and 3 piles total?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For their first turn, player 1 could take anywhere from 1 to 10 items out of any of the 3 piles.  So there are 30 different actions they could take.  Subsequent possible actions would depend on the move of player 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Find a way to improve the provided Nim game learning model. Do you think one can beat the Guru player?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint, choice\n",
    "\n",
    "# The number of piles is 3\n",
    "\n",
    "\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game():\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(st):\n",
    "    xored = st[0] ^ st[1] ^ st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(st)\n",
    "    #\n",
    "    for pile in range(3):\n",
    "        s = st[pile] ^ xored\n",
    "        if s <= st[pile]:\n",
    "            return st[pile]-s, pile\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_qlearner(_st):\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "\n",
    "    return move, pile  # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
    "\n",
    "def game(a, b):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[a] if side == 'A' else Engines[b]\n",
    "        move, pile = engine(state)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n, a, b):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for i in range(_n):\n",
    "        wins[game(a, b)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {a:>8s}{wins['A']:5d}  {b:>8s}{wins['B']:5d}\")\n",
    "\n",
    "    return wins['A'], wins['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable, Alpha, Gamma, Reward, Loss = None, 1.0, 0.8, 100.0, -100.0\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n):\n",
    "    global qtable\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True:  # while game not finished              \n",
    "            # make a random move - exploration\n",
    "            move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            \n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update(Reward, st1, move, pile, 0)  # I won\n",
    "                break  # new game\n",
    "       \n",
    "            else:\n",
    "                # find next best move based on state and qtable\n",
    "                move, pile = nim_qlearner(st2)    \n",
    "                st3 = st2\n",
    "                \n",
    "                # make the next move\n",
    "                st3[pile] -= move\n",
    "                \n",
    "                if st3 == [0, 0, 0]:\n",
    "                    qtable_update(Loss, st2, move, pile, 0)  # I lost\n",
    "                    break  # new game\n",
    "                    \n",
    "            qtable_update(0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            st1 = st2\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner  490    Random  510\n",
      "1000 games, Qlearner  558    Random  442\n",
      "1000 games, Qlearner  724    Random  276\n",
      "1000 games, Qlearner  823    Random  177\n",
      "1000 games, Qlearner  811    Random  189\n",
      "1000 games, Qlearner  828    Random  172\n",
      "1000 games, Qlearner  807    Random  193\n"
     ]
    }
   ],
   "source": [
    "# See the training size effect\n",
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "wins = []\n",
    "for n in n_train:\n",
    "    nim_qlearn(n)\n",
    "    a, b = play_games(1000, 'Qlearner', 'Random')\n",
    "    wins += [a/(a+b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From previous program:**\n",
    "\n",
    "Qlearner 508       Random 492\n",
    "\n",
    "Qlearner 561       Random 439\n",
    "\n",
    "Qlearner 713       Random 287\n",
    "\n",
    "Qlearner 728       Random 272\n",
    "\n",
    "Qlearner 717       Random 283\n",
    "\n",
    "Qlearner 719       Random 281\n",
    "\n",
    "Qlearner 731       Random 269"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These adjustments improve the Qlearner when training sizes are at 1000 games.  In this case, the Qlearner wins about 100 games more than the previous method.  This program checks an additional step than the original.  If a move does not result in a win, the next move is checked for a win.  If this move results in a win, the Qlearner gets a loss.  A penalty of -100 is then assigned to the Qtable for this move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
